# 迴歸模型：為什麼正則化如此重要？

**2024.10.15, 2024.10.22**

## 大綱

### 基礎概念

簡單線性迴歸的回顧：

- 模型描述及相關假設
- 最小平方法的優點
- **正規方程式**

平方和（Sum of squares）的分析：迴歸模型能解釋多少變異性？
- $SST = SSE + SSR$

信賴區間 vs. 預測區間

當存在序列相關性時...
- 杜賓--沃森統計量 (Durbin--Watson statistic)

### 多元線性迴歸 (MLR)

「線性」這個詞在線性迴歸模型中到底是什麼意思？

環境科學研究中常見的兩種等價表達式：
- $\textbf{y} = \textbf{X}\textbf{a} + \boldsymbol{\epsilon}$ （我們在課堂上會使用此公式）
- $\textbf{d} = \textbf{G}\textbf{m}$ （許多地球物理從業人員使用這個公式）

最小平方法的解：
- $\hat{\textbf{a}}=(\textbf{X}^\text{T}\textbf{X})^{-1}\textbf{X}^\text{T}\textbf{y}$

高斯--馬可夫定理與最佳線性無偏估計 (BLUE)

### 多元線性迴歸的進階探討

對於循環資料與類別資料，可以把它們轉換成可用於線性模型的形式

變異數分析 (ANOVA)
- 每個預測變數解釋的平方和
- F 檢定

#### 逐步迴歸

當有許多可能的預測變數時...
- 應該納入越多越好嗎？
- 應該納入少越好嗎？
- 如果我們不得不保留所有預測變數時該怎麼辦？

要如何進行逐步回歸？

批評：這不就是 **cherry-picking** 嗎？

秩不足的問題、病態問題、收縮方法家庭與正則化最小平方法

### 正則化最小平方法模型

正則化就是透過加上額外的限制項來重新設計損失函數。有那些好處呢？
- 在迴歸模型中保留所有預測變數。
- 讓迴歸模型的輸入滿秩或具有更佳的條件數，以改善解的不穩定度。

#### 嶺迴歸

又名脊迴歸、L-2 正則化，或是吉洪諾夫迴歸

嶺迴歸的兩種表達形式：隱式（拉格朗日量，參見教科書附錄 B）與顯式

嶺迴歸的解：
- $\hat{\textbf{a}}=(\textbf{X}^\text{T}\textbf{X}+\lambda \textbf{I})^{-1}\textbf{X}^\text{T}\textbf{y}$

如何選擇 $\lambda$？

#### Lasso

又名套索迴歸、拉索迴歸，或是 L-1 正則化

由於損失函數不可微，Lasso 通常需要數值方法求解。

Lasso 與嶺迴歸的最明顯區別：可以選擇預測變數

### 廣義最小平方法

探索更多重新設計損失函數的方法！

資料協方差矩陣與加權最小平方法

## 小組討論 & 展示主題

1. 期末專題概述  <!-- : an example https://ucb-stat-159-s23.github.io/project-Group28/README.html  -->
2. 線上專題成果網頁的展示 (就是本課程大綱啦)