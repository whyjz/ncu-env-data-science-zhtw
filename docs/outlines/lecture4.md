# 神經網路：要怎麼相信看似黑盒子的複雜模型？

## 課程大綱

神經網路 (Neural Network; NN)：源自生物學的發想，以及與資料科學的關聯

### 感知器 (Perceptrons)

神經網路的最基本結構！

**架構**：

- （對只有單一輸出變數 $y$ 的模型）$\hat{y} = f\Big(\sum_i w_i x_i + b\Big)$

- **激勵函數** $f$：
  - Heaviside 階梯函數（單位階躍函數）
  - 邏輯斯諦函數（或 $tanh$）
  - 線性整流函式（ReLU）
  - Softplus

感知器好用嗎？事實上僅適用於 *線性可分* 的問題。

### 多層感知器 (Multi-layer Perceptrons; MLP)

最簡單且實用的神經網路。

**架構**：

- （對只有單一輸出變數 $y$ 的模型）$h_j = f\Big(\sum_i^{m_1} w_{ji} x_i + b_j\Big),\quad \hat{y} = g\Big(\sum_j^{m_2} \tilde{w}_{kj} h_j + \tilde{b}_k\Big)$

- **隱藏層**：將線性感知器轉變為非線性模型的關鍵！
  - 模型具有許多參數。在神經網路的框架中，模型參數也稱為**權重**。
  - **超參數**：與架構相關的參數。例如：隱藏節點數量。

**損失函數**：最常見的選擇是均方誤差 (MSE)。

**求解器**：必須以數值方法求解。（見下文）

<!-- - 觀測資料量 vs 參數數量 -->

### 對非二次之損失函數進行最佳化 (aka 模型訓練)

非二次 (函數最高項次不是平方) 之損失函數 $J(\textbf{w})$ 在參數空間中可能有許多局部最小值，而且求得解析解幾乎是不可能的。不過我們還是可以利用數值方法求得單一個局部最小值，一個可行的策略為：

1. 找出損失函數最陡峭的下降方向
2. 往那個方向前進，但是要前進多少？

#### 牛頓法

此方法須先計算赫氏矩陣 (Hessian Matrix) $\textbf{H}_0$，這個矩陣包含 $J$ 的所有二階導數在 $\textbf{w}_0$ 這個座標點的值。每一次的迭代求解會依照此式更新模型參數：

$\textbf{w}_{k+1} = \textbf{w}_k - \textbf{H}_k^{-1} \nabla J(\textbf{w}_k)$

#### 梯度下降法

此方法將赫氏矩陣 $\textbf{H}_0$ 替換為固定或彈性的步長 $\eta$，通常稱為**學習率**。每一次的迭代求解會依照此式更新模型參數：

$\textbf{w}_{k+1} = \textbf{w}_k - \eta \nabla J(\textbf{w}_k)$

#### 梯度下降法的變體

「動量梯度下降」和「隨機梯度下降」 (與 Mini-batch)

#### 什麼時候結束訓練？

期 (Epoch) 的意思

什麼時候結束？
1. 由訓練集觀之
2. 由驗證集觀之 (提前停止法)

### 最佳化 MLP 神經網路

架構帶來的挑戰：
- 維度詛咒？

求解器帶來的挑戰：
- 最佳化過程會根據初始權重選擇而陷入局部最小值。

如何應對這些挑戰？集成方法 (ensemble approach)？正則化？

如果節點數量足夠大，即使只有一個隱藏層，我們也能做到萬用逼近 (Universal approximation)！

#### 反向傳播 (Back-Propagation)

克服維度詛咒，神經網路模型的救星！沒有它，根本沒辦法求解。

教科書摘錄：「如今，『反向傳播』一詞的使用有些模稜兩可。」

- 它可能指原始的反向傳播演算法（使用效率較低的梯度下降）；
- 或者，更可能是指僅使用反向誤差傳播的第一部分來計算 $J$ 的梯度，再搭配其他更快更有效率的演算法執行。

<!-- ### 梯度下降法的問題

TBD -->

### 極限學習機 (Extreme learning Machine; ELM)

使用集成方法的概念，將非線性問題轉換回線性問題。

**架構**：與 MLP 相同，另外再多兩條規則，使其轉換為線性問題：
  1. 輸入層的權重初始值是隨機指定的，而且不會迭代更新。
  2. 隱藏層的激勵函數是恆等函數；$f(x) = x$。

**損失函數**：與 MLP 相同。

**求解器**：對每個集成成員尋找解析解，所有的解再取平均。

#### ELM 的優勢

有哪些優勢？
<!-- 如何決定隱藏層節點數 $L$？ -->
<!-- 跳躍連接 -->
<!-- ### 徑向基函數 (RBF)
這是將神經網路的非線性降低為線性問題的另一種方法。
為什麼稱為「徑向基」？我們將在核方法部分重新討論這個問題。 -->


### 設計損失函數

MSE 與 MAE

Variance and Bias Errors (方差與偏差)

**正則化** (嘿... 又碰面了!)
1. 加入 $\lambda$
2. 提前停止法也算是一種正則化

### 超參數的調校與驗證

- 網格搜尋
- 隨機搜尋
- 驗證
- 交叉驗證
- 雙重交叉驗證


<!-- ### 其他集成方法
幫助減少變異數誤差（第 8.3 節）
- Bagging
- Stacking -->

## 最後的問題

**要怎麼相信看似黑盒子的複雜模型？**
- 理解架構，理解模型限制
- 因為損失函數具有許多局部極小值，要注意單次訓練神經網路模型時最終抵達的參數往往不會是最佳參數
- 驗證要做！
- 不要相信 (!?)
- 還有其他可能性嗎？

## 小組討論與示範

1. 確認分組 & 期末專題主題的討論
2. 製作期末 html 檔案的簡單方法

<!-- 1. 期末專題輸出概覽：範例 https://ucb-stat-159-s23.github.io/project-Group28/README.html 
2. 另一個範例是這個課程網頁。 -->